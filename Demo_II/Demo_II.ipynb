{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import datashade\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray\n",
    "\n",
    "hv.extension('bokeh', width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openeo API Demo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microservice Architektur\n",
    "\n",
    "<img src=\"./microservices.png\" height=70% width=70%></img>\n",
    "\n",
    "### Vorbereitung\n",
    "Bevor ich die unten stehenden Docker Befehle ausführen konnte habe ich folgendes gemacht:\n",
    "\n",
    "1. Github Repository klonen\n",
    "\n",
    "Ich habe das Github Repository [FelixGI1516/GeosSoftII_Frontend](https://github.com/FelixGI1516/GeoSoftII_Frontend) mittels `git clone` geklont.\n",
    "\n",
    "2. Docker File ausführen\n",
    "Ich bin in den Ordner mit dem Dockerfile gegangen und habe ein Docker image namens myserver gebaut:  \n",
    "```\n",
    "docker build -t myserver ./\n",
    "```\n",
    "\n",
    "3. Docker Container erzeugen\n",
    "Dann habe ich einen Container basierend auf myserver erzeugt:\n",
    "```\n",
    "docker run --name fe_server_i -p 80:80/ myserver\n",
    "```\n",
    "Wichtig: Nicht den Port im Befehl vergessen (Ist mir passiert :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker start fe_server_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Antworten des Servers in der Übersicht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load in Capabilities Response JSON\n",
    "x = requests.get('http://localhost/api/v1/')\n",
    "capabilities = 'Capabilities:\\n \\n'+ json.dumps(x.json(),indent=1) + '\\n \\n'\n",
    "\n",
    "print(capabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load in Collections Response JSON\n",
    "y = requests.get('http://localhost/api/v1/collections')\n",
    "collections  = 'Collections:\\n \\n'+ json.dumps(y.json(),indent=1) + '\\n \\n'\n",
    "\n",
    "print(collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load in Processes Response JSON\n",
    "z = requests.get('http://localhost/api/v1/processes')\n",
    "processes  = 'Processes:\\n \\n'+ json.dumps(z.json(),indent=1) + '\\n \\n'\n",
    "\n",
    "print(processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load in Processes Response JSON\n",
    "a = requests.get('http://localhost/api/v1/jobs')\n",
    "jobs  = 'Jobs:\\n \\n'+ json.dumps(a.json(),indent=1) + '\\n \\n'\n",
    "#Provozieren eines Errors\n",
    "ae = requests.get('http://localhost/api/v2/jobs')\n",
    "error = 'Error:\\n \\n'+ json.dumps(ae.json(),indent=1) + '\\n \\n'\n",
    "print(jobs, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend Validator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load in wellknown Response JSON\n",
    "b = requests.get('http://localhost/api/v1/.well-known/openeo')\n",
    "well_know  = 'Processes:\\n \\n'+ json.dumps(b.json(),indent=1) + '\\n \\n'\n",
    "\n",
    "print(well_know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"./validator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dem Ordner in dem wir uns nun befinden ist eine Datei namens `validae.cmd`.\\\n",
    "Diese führen wir aus und erhalten eine neue JSON-Datei namens `validator_output`.\\\n",
    "Das Ergebnis ist ein JSON, welches die Kompabilität unserer Endpunkte mit den Vorschriften von openeo vergleicht und bewertet.\n",
    "\n",
    "Die Voraussetzungen für die Installation des findet ihr Backend Validator [hier](https://github.com/Open-EO/openeo-backend-validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! validate\n",
    "with open(\"validator_output.json\") as v_output:\n",
    "    val = json.load(v_output)\n",
    "    print(json.dumps(val,indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker stop fe_server_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDVI Demo\n",
    "\n",
    "Das Problem ist das Datenformat SAFE in xarray nicht ohne Weiteres nutzbar ist.\n",
    "Deshalb haben wir eine Methode entwicklet, die mittels einiger Zwischenschritte die Umwandlung in das lesbare NetCDF Format erlaubt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Zelle haben wir in der Demo gezeigt, allerdings läuft sie sehr lange und hatte wir hatten etwas Schwierigkeiten sie wieder zum Laufen zu bringen. \n",
    "Deshalb wird hier nur noch auf den Ergebnis Datacube zugegriffen.\n",
    "```python\n",
    "#Umwandlung der SAFE Datei und Erzeugung des NetCDF Datacube\n",
    "%run NDVI\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../demodata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Öffnen des Datensatzes und Repräsentation als Grauwert Bild\n",
    "DS = xarray.open_dataset('datacube_NDVI.nc')\n",
    "DS\n",
    "ndvi_ds = hv.Dataset(DS, kdims=['null','Band', 'null','X', 'Y'])\n",
    "ndvi = ndvi_ds.to(hv.QuadMesh, kdims=[\"X\", \"Y\"])\n",
    "#%opts Greys [width=900 height=600]\n",
    "datashade(ndvi, precompute=True, cmap=plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die SST Berechnung wurde mit Dask verfeinert und kann mit nun eine Räumliche Auswahl treffen. \n",
    "Dies geschieht ünber einen neuen Parameter der eine Bounding Box von Koordinaten entgegen nihmt.\n",
    "\n",
    "Außerdem wurde die visuelle Repräsentation des Ergbenis überarbeitet. Diese kann in bestimmten Maßen ausgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../Demo_II/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../demodata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die verbesserte Visualisierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = mean_sst(['1989-01-01','1989-01-01'])\n",
    "\n",
    "# visualisieren\n",
    "\n",
    "sst_ds_day = hv.Dataset(x, kdims=['lon', 'lat'])\n",
    "sst_day = sst_ds_day.to(hv.QuadMesh, kdims=[\"lon\", \"lat\"])\n",
    "%opts RGB [width=900 height=600]\n",
    "datashade(sst_day, precompute=True, cmap=plt.cm.RdBu_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Räumliche Auswahl der Nord- und Ostsee:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mean_sst(['1989-10-01','1989-11-01'], [0, 50, 30, 75])\n",
    "\n",
    "# visualisieren\n",
    "\n",
    "# lon werte von 0 - 360 in -180 - 180 umwandeln\n",
    "lon = x['lon'].values\n",
    "lon = [min360(item) for item in lon]\n",
    "\n",
    "# lon Werte im dataset durch neue ersetzten, sonst kommt was komisches raus\n",
    "x = x.assign_coords(coords={\"lat\": x['lat'], \"lon\": lon})\n",
    "\n",
    "sst_x = hv.Dataset(x, kdims=['lon', 'lat'])\n",
    "sst_x = sst_x.to(hv.QuadMesh, kdims=[\"lon\", \"lat\"])\n",
    "%opts RGB [width=900 height=600]\n",
    "datashade(sst_x, precompute=True, cmap=plt.cm.RdBu_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "product_owner",
   "language": "python",
   "name": "product_owner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
