{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import mean_sst\n",
    "import mean_sst_dask\n",
    "import requests as rq\n",
    "import json\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server, Daten und SST\n",
    "\n",
    "Die erste Version eines `docker-compose.yml` ist jetzt fertig und verbindet die API mit dem SST Prozess. Außerdem funktioniert jetzt das Job Mangement des Servers, um diesen Prozess zu starten.\n",
    "\n",
    "### Durchlauf eines Jobs\n",
    "\n",
    "1. Zuerst startet man den Server über:\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "(in dem Verzeichnis, wo dieses Notebook liegt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ein Job JSON erstellen:\\\n",
    "Ein Beispiel JSON für einen SST Job (freundlicher Weise vom Server Team zur Verfügung gestellt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testjob = {\n",
    "  \"title\": \"Example Title\",\n",
    "  \"description\": \"Example Description\",\n",
    "  \"process\": {\n",
    "    \"process_graph\": {\n",
    "      \"loadcollection1\": {\n",
    "        \"process_id\": \"load_collection\",\n",
    "        \"arguments\": {\n",
    "          \"timeframe\" : [\"01-12-1981 00:00:00\",\"30-12-1981 00:00:00\",\"%d-%m-%Y %H:%M:%S\"],\n",
    "          \"DataType\": \"SST\"\n",
    "        }\n",
    "        },\n",
    "        \"SST\": {\n",
    "        \"process_id\": \"mean_sst\",\n",
    "        \"arguments\": {\n",
    "          \"data\":{\n",
    "              \"from_node\": \"loadcollection1\"\n",
    "          },\n",
    "          \"timeframe\":[\"1981-12-01\",\"1981-12-17\"],\n",
    "          \"bbox\":[-999,-999,-999,-999]\n",
    "          }\n",
    "        },\n",
    "        \"save\":{\n",
    "            \"process_id\": \"save_result\",\n",
    "            \"arguments\":{\n",
    "                \"SaveData\":{\n",
    "                    \"from_node\":\"SST\"\n",
    "                },\n",
    "                \"Format\": \"netcdf\"\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "      }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Den Job mittels HTTP Post an den jobs Endpoint schicken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Den Testdaten an /jobs Posten\n",
    "rq.post(\"http://localhost/api/v1/jobs\", json=testjob, headers={\"Content-Type\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Die Id des Jobs erfragen, in dem eine GET Anfrage an den /jobs Endpoint gestellt wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = rq.get(\"http://localhost/api/v1/jobs\")\n",
    "rjson = j.json()\n",
    "# Die Id des neusten Job abspeichern für spätere Verwendung\n",
    "job_id = rjson['jobs'][-1]['id']\n",
    "rjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Den Job ausführen über eine POST Anfrage an den results Endpoint des Jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rq.post(\"http://localhost/api/v1/jobs/\" + job_id + \"/results\" , json=None, headers={\"Content-Type\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Verlauf des Jobs im Server:\n",
    "\n",
    "Was im der Server im Hintergund macht: \n",
    "\n",
    "```bash\n",
    "frontend_1        | 172.21.0.1 - - [20/Jan/2021 12:14:29] \"POST /api/v1/jobs/051a9bc0-5b19-11eb-a9c8-0242ac150006/results HTTP/1.1\" 204 -\n",
    "frontend_1        | 172.21.0.5 - - [20/Jan/2021 12:14:32] \"GET /jobRunning/051a9bc0-5b19-11eb-a9c8-0242ac150006 HTTP/1.1\" 200 -\n",
    "database_1        | 172.21.0.5 - - [20/Jan/2021 12:14:32] \"POST /doJob/051a9bc0-5b19-11eb-a9c8-0242ac150006 HTTP/1.1\" 200 -\n",
    "database_1        | 172.21.0.5 - - [20/Jan/2021 12:14:32] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "...\n",
    "database_1        | 172.21.0.5 - - [20/Jan/2021 12:16:37] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:37] \"POST /doJob/051a9bc0-5b19-11eb-a9c8-0242ac150006 HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:37] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:42] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:47] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:52] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "sst_0_1           | 172.21.0.5 - - [20/Jan/2021 12:16:57] \"GET /jobStatus HTTP/1.1\" 200 -\n",
    "frontend_1        | 172.21.0.5 - - [20/Jan/2021 12:16:57] \"POST /takeData/051a9bc0-5b19-11eb-a9c8-0242ac150006 HTTP/1.1\" 200 -\n",
    "```\n",
    "- Zuerst übergibt der `frontend` container, den Job an den `database` container. \\\n",
    "- Dannach wird vom `database` container der entsprechende Datacube vorbereitet. \\\n",
    "- Dann wird übernimmt der `sst` container und berechnet den SST für den ausgewählten Zeitraum. \\\n",
    "- Das Ergebnis wird dann dem `/takeData` Endpoint des `frontend` containers übergeben und das Ergebnis zum Download bereitgestellt.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Den Downloadlink über eine GET Anfrage an /jobs results bekommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rq.get(\"http://localhost/api/v1/jobs/\" + job_id + \"/results\" )\n",
    "dl = res.json()[\"assets\"]\n",
    "dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Link im Browser eingeben und Ergebnis als netCDF herunterladen: http://localhost:80/download/< Euer Downloadlink>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fin = xr.open_dataset('../demodata/25ff212c-5b2c-11eb-a863-0242ac160005.nc') # Hier ggf. euren eigenen Pfad zum DOwnload einfügen\n",
    "fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( 9. Mit `docker-compose down -v` die lokale Umgebung aufräumen)\n",
    "\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask SST Laufzeitanalyse\n",
    "---\n",
    "*von Alexia*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The Dask delayed function decorates your functions so that they operate lazily. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph.\" (https://docs.dask.org/en/latest/delayed.html)\n",
    "-  Ein einfacher Weg Code, der nicht voneinander anhängt, parallel auszuführen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Dask divides arrays into many small pieces, called chunks, each of which is presumed to be small enough to fit into memory.\" \n",
    "\"If your chunks are too small, queueing up operations will be extremely slow, because Dask will translate each operation into a huge number of operations mapped across chunks. Computation on Dask arrays with small chunks can also be slow, because each operation on a chunk has some fixed overhead from the Python interpreter and the Dask task executor. Conversely, if your chunks are too big, some of your computation may be wasted, because Dask only computes results one chunk at a time.\" (http://xarray.pydata.org/en/stable/dask.html#chunking-and-performance)\n",
    "-  Erlaubt es ein Dataset in Stücke zu teilen, auf denen Operationen parallel angewendet werden können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dask Cluster'''\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Monate, kleines Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed und chunks => keine Parallelisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\")\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mit Dask delayed, ohne chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\")\n",
    "\n",
    "x = mean_sst_dask.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dauert länger, da dask delayed overhead erzeugt, der code an sich damit aber nicht parallelisiert werden kann, da er aufeinander aufbaut "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed, mit chunks={\"time\": \"auto\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\", chunks={\"time\": \"auto\"})\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "das verwenden wir. \"auto\" findet die beste chunk-größe automatisch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed, mit chunks={\"lat\": 200, \"lon\": 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\", chunks={\"lat\": 200, \"lon\": 400})\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "schlecht gewählte chunk-größe, hier manuell vorgegeben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed, mit chunks={\"lat\": \"auto\", \"lon\": \"auto\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\", chunks={\"lat\": \"auto\", \"lon\": \"auto\"})\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunken von lat und lon an sich scheint nicht ideal zu sein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mit Dask delayed, mit chunks={\"time\": \"auto\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem, dieser fall verbraucht zu viel memory: distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\", chunks={\"time\": \"auto\"})\n",
    "\n",
    "x = mean_sst_dask.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'], [0, -50, 60, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Monate, kein Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed und chunks => keine Parallelisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\")\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne Dask delayed, mit chunks={\"time\": \"auto\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(\"../demodata/sst.day.mean.1989.nc\", chunks={\"time\": \"auto\"})\n",
    "\n",
    "x = mean_sst.wrapper_mean_sst(ds, ['1989-01-01','1989-12-31'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Abstand zwischen den Laufzeiten von dem Fall ohne Parallelsisierung und dem Fall chunks={\"time\": \"auto\"} nimmt mit zunehmender größe des datasets zu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen \n",
    "\n",
    "Inzwischen gibt es zwei fertige Github Actions, im [Testrepo](https://github.com/GeoSoftII2020-21/TestRepo/actions).\n",
    "So soll auch das One Click Demo Szenario aus dem Pflichtenheft umgesetzt werden.\n",
    "\n",
    "###### [1. Pytest Workflow](https://github.com/GeoSoftII2020-21/TestRepo/actions?query=workflow%3A%22Pytest+Workflow%22)\n",
    "\n",
    "Diese Action soll alle Submodule des Projekts ihre Unittests mit Pytest ausführen lassen. Dazu werden zuerst alle Teilprojekte geupdated. Dann wird auf in einem zweiten Schritt Pytest über alle gesammelten Unitests ausgeführt.\n",
    "\n",
    "🚧 *Momentan gibt es noch ein Problem mit den Testdatensätzen, die für manche Tests verwendet wurden. Da diese, aufgrund ihrer Größe, nicht auf Github liegen, schlägt die Action bis jetzt noch fehl. An einer Lösung wird gearbeitet.*\n",
    "\n",
    "##### [2. Backendvalidator](https://github.com/GeoSoftII2020-21/TestRepo/actions?query=workflow%3ABackend-Validator)\n",
    "\n",
    "Diese Action führt den openeo Backendvalidator(s. Demo II für die lokale Ausführung) nun über Github aus. Dabei wird das openeo Backend Validator Repository geklont und der validate befehl auf die den momentanen Stand der API ausgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "product_owner",
   "language": "python",
   "name": "product_owner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
